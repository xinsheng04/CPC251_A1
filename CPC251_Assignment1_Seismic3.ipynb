{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39d43a4e",
      "metadata": {
        "id": "39d43a4e"
      },
      "source": [
        "Note: Use this template to develop your project. Do not change the steps. For each step, you may add additional cells if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9b28a8",
      "metadata": {
        "id": "2b9b28a8"
      },
      "source": [
        "#### Group Information\n",
        "\n",
        "Group No:\n",
        "\n",
        "- Member 1: Tan Xin Sheng (22301827)\n",
        "- Member 2: Jee Ci Hong (22303833)\n",
        "- Member 3: Wong Zhi Heng (22304070)\n",
        "- Member 4: Chan Jia Liang (22304211)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0a3c45",
      "metadata": {
        "id": "2c0a3c45"
      },
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b84136",
      "metadata": {
        "id": "79b84136"
      },
      "outputs": [],
      "source": [
        "%config Completer.use_jedi=False\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.models import HoverTool, ColumnDataSource\n",
        "import numpy as np\n",
        "output_notebook()\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dceb4ff",
      "metadata": {
        "id": "5dceb4ff"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83d38c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a83d38c1",
        "outputId": "640c72b1-556d-40cc-caaf-75fa26dcc4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            f1         f2         f3         f4         f5  label\n",
              "0     1.286233  15.643743  -1.879915 -11.294839  15.245472      0\n",
              "1     2.853398   0.129878  17.620669   3.945204   8.157459      1\n",
              "2     3.285310   3.176560  12.610554  -6.063613   1.831887      0\n",
              "3     2.019516  -1.967793   9.306435  -0.938714  -1.203038      0\n",
              "4    -2.326527   3.453234  13.855478  -5.236421   1.547216      0\n",
              "..         ...        ...        ...        ...        ...    ...\n",
              "995  -2.248262  -4.619586   3.248760   9.114543   4.370790      1\n",
              "996   7.882330   1.942559  13.304597  -2.682707   0.623444      0\n",
              "997  14.421812 -10.688891   5.242771  -2.954794  11.689658      1\n",
              "998   5.566459  -4.118762   3.670333   7.948329  10.940144      1\n",
              "999   0.861633 -10.222440  11.606533  -0.388557   4.041448      1\n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10579a58-bb39-4e1a-8943-458ddf798a89\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.286233</td>\n",
              "      <td>15.643743</td>\n",
              "      <td>-1.879915</td>\n",
              "      <td>-11.294839</td>\n",
              "      <td>15.245472</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.853398</td>\n",
              "      <td>0.129878</td>\n",
              "      <td>17.620669</td>\n",
              "      <td>3.945204</td>\n",
              "      <td>8.157459</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.285310</td>\n",
              "      <td>3.176560</td>\n",
              "      <td>12.610554</td>\n",
              "      <td>-6.063613</td>\n",
              "      <td>1.831887</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.019516</td>\n",
              "      <td>-1.967793</td>\n",
              "      <td>9.306435</td>\n",
              "      <td>-0.938714</td>\n",
              "      <td>-1.203038</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.326527</td>\n",
              "      <td>3.453234</td>\n",
              "      <td>13.855478</td>\n",
              "      <td>-5.236421</td>\n",
              "      <td>1.547216</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-2.248262</td>\n",
              "      <td>-4.619586</td>\n",
              "      <td>3.248760</td>\n",
              "      <td>9.114543</td>\n",
              "      <td>4.370790</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>7.882330</td>\n",
              "      <td>1.942559</td>\n",
              "      <td>13.304597</td>\n",
              "      <td>-2.682707</td>\n",
              "      <td>0.623444</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>14.421812</td>\n",
              "      <td>-10.688891</td>\n",
              "      <td>5.242771</td>\n",
              "      <td>-2.954794</td>\n",
              "      <td>11.689658</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>5.566459</td>\n",
              "      <td>-4.118762</td>\n",
              "      <td>3.670333</td>\n",
              "      <td>7.948329</td>\n",
              "      <td>10.940144</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.861633</td>\n",
              "      <td>-10.222440</td>\n",
              "      <td>11.606533</td>\n",
              "      <td>-0.388557</td>\n",
              "      <td>4.041448</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10579a58-bb39-4e1a-8943-458ddf798a89')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-10579a58-bb39-4e1a-8943-458ddf798a89 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-10579a58-bb39-4e1a-8943-458ddf798a89');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b79ad152-c7f2-41e9-84e0-e37ef294ad79\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b79ad152-c7f2-41e9-84e0-e37ef294ad79')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b79ad152-c7f2-41e9-84e0-e37ef294ad79 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5011a554-b06e-406c-8914-d6876d44b70e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5011a554-b06e-406c-8914-d6876d44b70e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.105038036847515,\n        \"min\": -16.877003479653073,\n        \"max\": 15.37667324245604,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          7.478655269051763,\n          -0.1534670303384004,\n          0.9861794549861752\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.983216780191279,\n        \"min\": -18.725112020300365,\n        \"max\": 17.904489749046576,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          -5.496001282905993,\n          -1.8630197206177292,\n          0.1448044017860768\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.8400739696226465,\n        \"min\": -16.25580352284886,\n        \"max\": 24.342183999217767,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          4.74245777702824,\n          11.136525146744749,\n          15.066038898076062\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.439400732315159,\n        \"min\": -13.320196438539396,\n        \"max\": 20.199926797039105,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          7.669796237283062,\n          2.4303591876061543,\n          1.2671448790276658\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.186770329586526,\n        \"min\": -5.700802583407687,\n        \"max\": 20.973490901981503,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          2.3034259297227666,\n          -1.690578836378704,\n          -1.4805395154001282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "url=\"https://raw.githubusercontent.com/xinsheng04/CPC251_A1/refs/heads/main/classification_dataset.csv\"\n",
        "dataset = pd.read_csv(url)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0f56c6",
      "metadata": {
        "id": "7f0f56c6"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4024775",
      "metadata": {
        "id": "b4024775"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y, yhat):\n",
        "    \"\"\"\n",
        "    This function calculates the loss function\n",
        "    \"\"\"\n",
        "    # Ensure consistent data types\n",
        "    y = tf.cast(y, tf.float32)\n",
        "    y = tf.reshape(y, (-1, 1))\n",
        "    yhat = tf.cast(yhat, tf.float32)\n",
        "    # We clip all yhat values by a small amount to remove any absolute 0. This is because log(0) is undefined and will cause the loss fn to return nan\n",
        "    epsilon = 1e-7  # to avoid log(0)\n",
        "    yhat = tf.clip_by_value(yhat, epsilon, 1 - epsilon)\n",
        "    # The loss fn is binary cross-entropy\n",
        "    J = tf.reduce_mean(-((y * tf.math.log(yhat) / tf.math.log(2.0)) + ((1 - y) * tf.math.log(1 - yhat) / tf.math.log(2.0))))\n",
        "    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753a2eca",
      "metadata": {
        "id": "753a2eca"
      },
      "source": [
        "#### Define function to perform prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdbf2168",
      "metadata": {
        "id": "bdbf2168"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    This function calculates the sigmoid function.\n",
        "    \"\"\"\n",
        "    sigm = 1/(1+tf.exp(-z))\n",
        "    return sigm\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"\n",
        "    This function calculates the ReLU function.\n",
        "    \"\"\"\n",
        "    relu = tf.maximum(0.0,z)\n",
        "    return relu\n",
        "\n",
        "def forward(W, B, x):\n",
        "    \"\"\"\n",
        "    This function calculates the forward pass (predicts the label).\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - W: list of weight tensors [W0, W1]\n",
        "         W[0] shape: (nneurons, nfeatures) -> input to hidden layer\n",
        "         W[1] shape: (nneurons,)   -> hidden to output\n",
        "    - B: list of bias tensors [B0, B1]\n",
        "         B[0] shape: (nneurons,)   -> hidden layer bias\n",
        "         B[1] shape: ()     -> scalar bias for output\n",
        "    - x: input vector, shape (5,)\n",
        "    \"\"\"\n",
        "    # Ensure consistent data types\n",
        "    W[0] = tf.cast(W[0], tf.float32)\n",
        "    W[1] = tf.cast(W[1], tf.float32)\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    B[0] = tf.cast(B[0], tf.float32)\n",
        "    B[1] = tf.cast(B[1], tf.float32)\n",
        "\n",
        "    neuron_vals=[] # We start with an empty list then append predictions. Finally, we convert it to a tensor. This method avoids breaking gradient tape\n",
        "\n",
        "    # Compute the prediction for hidden layer\n",
        "    # shape can be used to obtain the number of hidden units. The dimension of W[0] is nneurons x nfeatures.\n",
        "    for neuron in range(W[0].shape[0]):\n",
        "      # logits = W . x + B\n",
        "      z = tf.linalg.matvec(x, W[0][neuron]) + B[0][neuron]\n",
        "\n",
        "      neuron_vals.append(relu(z))\n",
        "\n",
        "    neuron_yhat = tf.stack(neuron_vals) #Shape: (nneurons,)\n",
        "\n",
        "    # Compute the prediction for the output layer\n",
        "    z = tf.linalg.matvec(tf.transpose(neuron_yhat), W[1]) + B[1]\n",
        "    yhat = sigmoid(z)\n",
        "    return yhat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bc735b",
      "metadata": {
        "id": "e7bc735b"
      },
      "source": [
        "#### Define function for model training\n",
        "Display the training and validation loss values for each epoch of the training loop. The displayed value must be in 6 decimal places.<br>\n",
        "Hint: <br>\n",
        "Use `tf.GradientTape` to compute the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe17ccc5",
      "metadata": {
        "id": "fe17ccc5"
      },
      "outputs": [],
      "source": [
        "def train(W, B, x, y, alpha):\n",
        "    \"\"\"\n",
        "    This function performs the forward pass, computes the gradient and update the weights and biases.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Loss fn and forward fn are used to compute J with respect to W and B\n",
        "        yhat = forward(W, B, x)\n",
        "        J = loss_fn(y, yhat)\n",
        "    dJ_dW, dJ_dB = tape.gradient(J, [W, B]) # Compute gradient\n",
        "    W[0].assign_sub(alpha * dJ_dW[0])  # Update W0 (input to hidden layer)\n",
        "    W[1].assign_sub(alpha * dJ_dW[1])  # Update W1 (hidden to output)\n",
        "    B[0].assign_sub(alpha * dJ_dB[0])  # Update B0 (hidden layer biases)\n",
        "    B[1].assign_sub(alpha * dJ_dB[1])  # Update B1 (output layer bias)\n",
        "    return [W, B]\n",
        "\n",
        "def fit(model, train_ds, vald_ds, batch_size, alpha, max_epochs):\n",
        "    \"\"\"\n",
        "    This function implements the training loop.\n",
        "    \"\"\"\n",
        "    train_loss_per_epoch = []\n",
        "    val_loss_per_epoch = []\n",
        "    # Early stopping regularization\n",
        "    j = 0 # Patience counter\n",
        "    v = float('inf') # best validation loss\n",
        "    patience = 20 # Maximum amount of patience permitted before training is stopped early\n",
        "    # Best weight and bias parameters\n",
        "    w_best = model[0]\n",
        "    b_best = model[1]\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        # Train model and compute training loss\n",
        "        for x_batch_train, y_batch_train in train_ds.batch(batch_size):\n",
        "            model = train(model[0], model[1], x_batch_train, y_batch_train, alpha)\n",
        "\n",
        "        for x_batch_train, y_batch_train in train_ds.batch(batch_size):\n",
        "            yhat = forward(model[0], model[1], x_batch_train)\n",
        "            train_loss += loss_fn(y_batch_train, yhat)\n",
        "            train_batches += 1\n",
        "\n",
        "        # train loss per epoch is average train loss per batch\n",
        "        train_loss_mean = train_loss/train_batches\n",
        "        train_loss_per_epoch.append(train_loss_mean)\n",
        "\n",
        "        # Compute validation loss\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        for x_batch_val, y_batch_val in vald_ds.batch(batch_size):\n",
        "          yhat = forward(model[0], model[1], x_batch_val)\n",
        "          val_loss += loss_fn(y_batch_val, yhat)\n",
        "          val_batches += 1\n",
        "\n",
        "        # val loss per epoch is average val loss per batch\n",
        "        val_loss_mean = val_loss/val_batches\n",
        "        val_loss_per_epoch.append(val_loss_mean)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss_mean:.4f}, Val Loss = {val_loss_mean:.4f}\")\n",
        "\n",
        "        if(val_loss_mean < v):\n",
        "          # Update w_best, b_best, reset j to 0, update v\n",
        "          j = 0\n",
        "          w_best = model[0]\n",
        "          b_best = model[1]\n",
        "          v = val_loss_mean\n",
        "        else:\n",
        "          j += 1 # increase j counter\n",
        "        if(j > patience):\n",
        "          # end the training cycle early if the validation loss is not improving\n",
        "          print(f\"Early stopping of training cycle due to validation loss value not improving. Patience = {patience}\")\n",
        "          break\n",
        "\n",
        "    model = [w_best, b_best]\n",
        "    return model, train_loss_per_epoch, val_loss_per_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f45213",
      "metadata": {
        "id": "28f45213"
      },
      "source": [
        "#### Define the tensors to hold the weights and biases (create the model)\n",
        "Hint: <br>\n",
        "Use `tf.Variable` to create the tensors.<br>\n",
        "Put the tensors in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e2172b",
      "metadata": {
        "id": "a2e2172b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec029cf5-a6b7-46e2-94f2-aa5e0436c164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W = [[-0.75003463 -0.9193431   0.30809     0.78508955 -1.0827471 ]\n",
            " [ 1.2571588  -0.25276953  0.9932638  -0.16744997  0.82432914]\n",
            " [ 1.9697998   0.8349025   1.8573571   0.00243461  0.7833899 ]\n",
            " [-0.86306304 -0.84122187  1.0752046  -0.10224541  1.9814366 ]\n",
            " [-0.38581765 -0.6640211   0.41697913  0.6161078  -0.63865685]\n",
            " [-0.48527357 -0.7884209   1.4255979   0.07700045  0.4904633 ]\n",
            " [-1.1251894  -0.7095601  -0.69189173  0.67671067 -1.0105356 ]\n",
            " [ 0.07639316 -1.3005447   0.60728616 -0.7284462   1.4135318 ]\n",
            " [-0.817176    0.8899839  -2.1699095  -0.549517   -0.5309505 ]\n",
            " [-0.13242227 -0.01786664  1.2714607  -0.44251543  0.26207814]] [-1.5443022   0.6397835  -0.07624615  0.17967835  0.12704368  0.46972516\n",
            " -0.39366126  0.12492722 -2.459607    0.07686502]\n",
            "\n",
            "B = [-0.3354743   1.7833872  -1.034346    1.1316265  -0.44773486  0.6653773\n",
            "  0.01819446  0.44361678  0.9504468  -1.6466304 ] [1.7567449]\n"
          ]
        }
      ],
      "source": [
        "nneurons = 10 # number of neurons\n",
        "nfeatures = 5 # 5 features: [f1, f2, f3, f4, f5] are used\n",
        "batch_size = 50\n",
        "alpha = 0.0001\n",
        "epochs = 500\n",
        "W = [\n",
        "    tf.Variable(np.random.randn(nneurons, nfeatures), dtype=tf.float32),  # Input to hidden layer\n",
        "    tf.Variable(np.random.randn(nneurons), dtype=tf.float32)     # Hidden to output layer\n",
        "]\n",
        "\n",
        "B = [\n",
        "    tf.Variable(np.random.randn(nneurons), dtype=tf.float32),  # Bias for hidden layer\n",
        "    tf.Variable(np.random.randn(1), dtype=tf.float32)   # Bias for output layer\n",
        "]\n",
        "model = [W, B]\n",
        "print(f\"W = {model[0][0].numpy()} {model[0][1].numpy()}\")\n",
        "print()\n",
        "print(f\"B = {model[1][0].numpy()} {model[1][1].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176badb8",
      "metadata": {
        "id": "176badb8"
      },
      "source": [
        "#### Split the dataset\n",
        "The ratio of training and test is 7:1:2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa1b9b6",
      "metadata": {
        "id": "5fa1b9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c4f011-d3b2-437a-a40c-1b7ef5ec6431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (1000, 5), Feature columns: Index(['f1', 'f2', 'f3', 'f4', 'f5'], dtype='object')\n",
            "Label shape: (1000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = dataset.drop(\"label\", axis=1)\n",
        "y = dataset[\"label\"]\n",
        "print(f\"Feature shape: {X.shape}, Feature columns: {X.columns}\")\n",
        "print(f\"Label shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_train, y_train, test_size=0.333, random_state=42)"
      ],
      "metadata": {
        "id": "4b4CSXm42Oum"
      },
      "id": "4b4CSXm42Oum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c3c4d6cf",
      "metadata": {
        "id": "c3c4d6cf"
      },
      "source": [
        "#### Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f689b7c6",
      "metadata": {
        "id": "f689b7c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a2e7d6",
      "metadata": {
        "id": "b1a2e7d6"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6304c496",
      "metadata": {
        "id": "6304c496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e3a994-36b2-4827-dac9-7553cd35d458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 3.3780, Val Loss = 3.3874\n",
            "Epoch 2: Train Loss = 3.3744, Val Loss = 3.3837\n",
            "Epoch 3: Train Loss = 3.3708, Val Loss = 3.3801\n",
            "Epoch 4: Train Loss = 3.3672, Val Loss = 3.3764\n",
            "Epoch 5: Train Loss = 3.3637, Val Loss = 3.3728\n",
            "Epoch 6: Train Loss = 3.3600, Val Loss = 3.3692\n",
            "Epoch 7: Train Loss = 3.3564, Val Loss = 3.3655\n",
            "Epoch 8: Train Loss = 3.3528, Val Loss = 3.3619\n",
            "Epoch 9: Train Loss = 3.3492, Val Loss = 3.3583\n",
            "Epoch 10: Train Loss = 3.3457, Val Loss = 3.3547\n",
            "Epoch 11: Train Loss = 3.3421, Val Loss = 3.3511\n",
            "Epoch 12: Train Loss = 3.3385, Val Loss = 3.3475\n",
            "Epoch 13: Train Loss = 3.3349, Val Loss = 3.3439\n",
            "Epoch 14: Train Loss = 3.3313, Val Loss = 3.3403\n",
            "Epoch 15: Train Loss = 3.3278, Val Loss = 3.3367\n",
            "Epoch 16: Train Loss = 3.3242, Val Loss = 3.3331\n",
            "Epoch 17: Train Loss = 3.3206, Val Loss = 3.3295\n",
            "Epoch 18: Train Loss = 3.3170, Val Loss = 3.3259\n",
            "Epoch 19: Train Loss = 3.3134, Val Loss = 3.3224\n",
            "Epoch 20: Train Loss = 3.3099, Val Loss = 3.3188\n",
            "Epoch 21: Train Loss = 3.3063, Val Loss = 3.3152\n",
            "Epoch 22: Train Loss = 3.3027, Val Loss = 3.3116\n",
            "Epoch 23: Train Loss = 3.2991, Val Loss = 3.3081\n",
            "Epoch 24: Train Loss = 3.2955, Val Loss = 3.3045\n",
            "Epoch 25: Train Loss = 3.2919, Val Loss = 3.3009\n",
            "Epoch 26: Train Loss = 3.2883, Val Loss = 3.2973\n",
            "Epoch 27: Train Loss = 3.2847, Val Loss = 3.2938\n",
            "Epoch 28: Train Loss = 3.2811, Val Loss = 3.2902\n",
            "Epoch 29: Train Loss = 3.2775, Val Loss = 3.2866\n",
            "Epoch 30: Train Loss = 3.2740, Val Loss = 3.2831\n",
            "Epoch 31: Train Loss = 3.2704, Val Loss = 3.2795\n",
            "Epoch 32: Train Loss = 3.2668, Val Loss = 3.2760\n",
            "Epoch 33: Train Loss = 3.2633, Val Loss = 3.2725\n",
            "Epoch 34: Train Loss = 3.2597, Val Loss = 3.2689\n",
            "Epoch 35: Train Loss = 3.2561, Val Loss = 3.2654\n",
            "Epoch 36: Train Loss = 3.2526, Val Loss = 3.2619\n",
            "Epoch 37: Train Loss = 3.2491, Val Loss = 3.2584\n",
            "Epoch 38: Train Loss = 3.2455, Val Loss = 3.2548\n",
            "Epoch 39: Train Loss = 3.2420, Val Loss = 3.2513\n",
            "Epoch 40: Train Loss = 3.2385, Val Loss = 3.2478\n",
            "Epoch 41: Train Loss = 3.2349, Val Loss = 3.2443\n",
            "Epoch 42: Train Loss = 3.2314, Val Loss = 3.2408\n",
            "Epoch 43: Train Loss = 3.2279, Val Loss = 3.2373\n",
            "Epoch 44: Train Loss = 3.2244, Val Loss = 3.2339\n",
            "Epoch 45: Train Loss = 3.2209, Val Loss = 3.2304\n",
            "Epoch 46: Train Loss = 3.2174, Val Loss = 3.2269\n",
            "Epoch 47: Train Loss = 3.2139, Val Loss = 3.2234\n",
            "Epoch 48: Train Loss = 3.2104, Val Loss = 3.2199\n",
            "Epoch 49: Train Loss = 3.2069, Val Loss = 3.2165\n",
            "Epoch 50: Train Loss = 3.2034, Val Loss = 3.2130\n",
            "Epoch 51: Train Loss = 3.1999, Val Loss = 3.2096\n",
            "Epoch 52: Train Loss = 3.1965, Val Loss = 3.2061\n",
            "Epoch 53: Train Loss = 3.1930, Val Loss = 3.2027\n",
            "Epoch 54: Train Loss = 3.1895, Val Loss = 3.1992\n",
            "Epoch 55: Train Loss = 3.1861, Val Loss = 3.1958\n",
            "Epoch 56: Train Loss = 3.1826, Val Loss = 3.1924\n",
            "Epoch 57: Train Loss = 3.1792, Val Loss = 3.1890\n",
            "Epoch 58: Train Loss = 3.1757, Val Loss = 3.1855\n",
            "Epoch 59: Train Loss = 3.1723, Val Loss = 3.1821\n",
            "Epoch 60: Train Loss = 3.1688, Val Loss = 3.1787\n",
            "Epoch 61: Train Loss = 3.1654, Val Loss = 3.1753\n",
            "Epoch 62: Train Loss = 3.1620, Val Loss = 3.1719\n",
            "Epoch 63: Train Loss = 3.1585, Val Loss = 3.1684\n",
            "Epoch 64: Train Loss = 3.1551, Val Loss = 3.1650\n",
            "Epoch 65: Train Loss = 3.1516, Val Loss = 3.1615\n",
            "Epoch 66: Train Loss = 3.1482, Val Loss = 3.1580\n",
            "Epoch 67: Train Loss = 3.1447, Val Loss = 3.1546\n",
            "Epoch 68: Train Loss = 3.1413, Val Loss = 3.1511\n",
            "Epoch 69: Train Loss = 3.1378, Val Loss = 3.1477\n",
            "Epoch 70: Train Loss = 3.1344, Val Loss = 3.1443\n",
            "Epoch 71: Train Loss = 3.1310, Val Loss = 3.1408\n",
            "Epoch 72: Train Loss = 3.1276, Val Loss = 3.1374\n",
            "Epoch 73: Train Loss = 3.1241, Val Loss = 3.1340\n",
            "Epoch 74: Train Loss = 3.1207, Val Loss = 3.1306\n",
            "Epoch 75: Train Loss = 3.1173, Val Loss = 3.1272\n",
            "Epoch 76: Train Loss = 3.1139, Val Loss = 3.1238\n",
            "Epoch 77: Train Loss = 3.1105, Val Loss = 3.1204\n",
            "Epoch 78: Train Loss = 3.1071, Val Loss = 3.1170\n",
            "Epoch 79: Train Loss = 3.1038, Val Loss = 3.1136\n",
            "Epoch 80: Train Loss = 3.1004, Val Loss = 3.1103\n",
            "Epoch 81: Train Loss = 3.0970, Val Loss = 3.1069\n",
            "Epoch 82: Train Loss = 3.0936, Val Loss = 3.1035\n",
            "Epoch 83: Train Loss = 3.0903, Val Loss = 3.1002\n",
            "Epoch 84: Train Loss = 3.0869, Val Loss = 3.0968\n",
            "Epoch 85: Train Loss = 3.0836, Val Loss = 3.0934\n",
            "Epoch 86: Train Loss = 3.0802, Val Loss = 3.0901\n",
            "Epoch 87: Train Loss = 3.0769, Val Loss = 3.0868\n",
            "Epoch 88: Train Loss = 3.0735, Val Loss = 3.0834\n",
            "Epoch 89: Train Loss = 3.0702, Val Loss = 3.0801\n",
            "Epoch 90: Train Loss = 3.0669, Val Loss = 3.0768\n",
            "Epoch 91: Train Loss = 3.0635, Val Loss = 3.0734\n",
            "Epoch 92: Train Loss = 3.0602, Val Loss = 3.0701\n",
            "Epoch 93: Train Loss = 3.0569, Val Loss = 3.0668\n",
            "Epoch 94: Train Loss = 3.0536, Val Loss = 3.0635\n",
            "Epoch 95: Train Loss = 3.0503, Val Loss = 3.0602\n",
            "Epoch 96: Train Loss = 3.0470, Val Loss = 3.0569\n",
            "Epoch 97: Train Loss = 3.0437, Val Loss = 3.0536\n",
            "Epoch 98: Train Loss = 3.0404, Val Loss = 3.0503\n",
            "Epoch 99: Train Loss = 3.0371, Val Loss = 3.0470\n",
            "Epoch 100: Train Loss = 3.0338, Val Loss = 3.0438\n",
            "Epoch 101: Train Loss = 3.0305, Val Loss = 3.0405\n",
            "Epoch 102: Train Loss = 3.0273, Val Loss = 3.0372\n",
            "Epoch 103: Train Loss = 3.0240, Val Loss = 3.0339\n",
            "Epoch 104: Train Loss = 3.0207, Val Loss = 3.0307\n",
            "Epoch 105: Train Loss = 3.0175, Val Loss = 3.0274\n",
            "Epoch 106: Train Loss = 3.0142, Val Loss = 3.0242\n",
            "Epoch 107: Train Loss = 3.0110, Val Loss = 3.0209\n",
            "Epoch 108: Train Loss = 3.0077, Val Loss = 3.0177\n",
            "Epoch 109: Train Loss = 3.0045, Val Loss = 3.0145\n",
            "Epoch 110: Train Loss = 3.0012, Val Loss = 3.0112\n",
            "Epoch 111: Train Loss = 2.9980, Val Loss = 3.0080\n",
            "Epoch 112: Train Loss = 2.9948, Val Loss = 3.0048\n",
            "Epoch 113: Train Loss = 2.9916, Val Loss = 3.0016\n",
            "Epoch 114: Train Loss = 2.9883, Val Loss = 2.9984\n",
            "Epoch 115: Train Loss = 2.9851, Val Loss = 2.9952\n",
            "Epoch 116: Train Loss = 2.9819, Val Loss = 2.9920\n",
            "Epoch 117: Train Loss = 2.9787, Val Loss = 2.9888\n",
            "Epoch 118: Train Loss = 2.9755, Val Loss = 2.9856\n",
            "Epoch 119: Train Loss = 2.9723, Val Loss = 2.9824\n",
            "Epoch 120: Train Loss = 2.9691, Val Loss = 2.9792\n",
            "Epoch 121: Train Loss = 2.9659, Val Loss = 2.9760\n",
            "Epoch 122: Train Loss = 2.9628, Val Loss = 2.9729\n",
            "Epoch 123: Train Loss = 2.9596, Val Loss = 2.9697\n",
            "Epoch 124: Train Loss = 2.9564, Val Loss = 2.9665\n",
            "Epoch 125: Train Loss = 2.9532, Val Loss = 2.9634\n",
            "Epoch 126: Train Loss = 2.9501, Val Loss = 2.9602\n",
            "Epoch 127: Train Loss = 2.9469, Val Loss = 2.9571\n",
            "Epoch 128: Train Loss = 2.9438, Val Loss = 2.9539\n",
            "Epoch 129: Train Loss = 2.9406, Val Loss = 2.9508\n",
            "Epoch 130: Train Loss = 2.9374, Val Loss = 2.9476\n",
            "Epoch 131: Train Loss = 2.9343, Val Loss = 2.9445\n",
            "Epoch 132: Train Loss = 2.9312, Val Loss = 2.9414\n",
            "Epoch 133: Train Loss = 2.9280, Val Loss = 2.9382\n",
            "Epoch 134: Train Loss = 2.9249, Val Loss = 2.9351\n",
            "Epoch 135: Train Loss = 2.9218, Val Loss = 2.9320\n",
            "Epoch 136: Train Loss = 2.9186, Val Loss = 2.9289\n",
            "Epoch 137: Train Loss = 2.9155, Val Loss = 2.9258\n",
            "Epoch 138: Train Loss = 2.9124, Val Loss = 2.9226\n",
            "Epoch 139: Train Loss = 2.9092, Val Loss = 2.9195\n",
            "Epoch 140: Train Loss = 2.9061, Val Loss = 2.9163\n",
            "Epoch 141: Train Loss = 2.9030, Val Loss = 2.9131\n",
            "Epoch 142: Train Loss = 2.8999, Val Loss = 2.9100\n",
            "Epoch 143: Train Loss = 2.8967, Val Loss = 2.9068\n",
            "Epoch 144: Train Loss = 2.8936, Val Loss = 2.9037\n",
            "Epoch 145: Train Loss = 2.8905, Val Loss = 2.9006\n",
            "Epoch 146: Train Loss = 2.8874, Val Loss = 2.8974\n",
            "Epoch 147: Train Loss = 2.8843, Val Loss = 2.8943\n",
            "Epoch 148: Train Loss = 2.8812, Val Loss = 2.8912\n",
            "Epoch 149: Train Loss = 2.8781, Val Loss = 2.8881\n",
            "Epoch 150: Train Loss = 2.8750, Val Loss = 2.8850\n",
            "Epoch 151: Train Loss = 2.8719, Val Loss = 2.8818\n",
            "Epoch 152: Train Loss = 2.8688, Val Loss = 2.8787\n",
            "Epoch 153: Train Loss = 2.8658, Val Loss = 2.8756\n",
            "Epoch 154: Train Loss = 2.8627, Val Loss = 2.8725\n",
            "Epoch 155: Train Loss = 2.8596, Val Loss = 2.8695\n",
            "Epoch 156: Train Loss = 2.8566, Val Loss = 2.8664\n",
            "Epoch 157: Train Loss = 2.8535, Val Loss = 2.8633\n",
            "Epoch 158: Train Loss = 2.8504, Val Loss = 2.8602\n",
            "Epoch 159: Train Loss = 2.8474, Val Loss = 2.8571\n",
            "Epoch 160: Train Loss = 2.8443, Val Loss = 2.8541\n",
            "Epoch 161: Train Loss = 2.8413, Val Loss = 2.8510\n",
            "Epoch 162: Train Loss = 2.8383, Val Loss = 2.8479\n",
            "Epoch 163: Train Loss = 2.8352, Val Loss = 2.8449\n",
            "Epoch 164: Train Loss = 2.8322, Val Loss = 2.8418\n",
            "Epoch 165: Train Loss = 2.8292, Val Loss = 2.8388\n",
            "Epoch 166: Train Loss = 2.8262, Val Loss = 2.8358\n",
            "Epoch 167: Train Loss = 2.8231, Val Loss = 2.8327\n",
            "Epoch 168: Train Loss = 2.8201, Val Loss = 2.8297\n",
            "Epoch 169: Train Loss = 2.8171, Val Loss = 2.8267\n",
            "Epoch 170: Train Loss = 2.8141, Val Loss = 2.8236\n",
            "Epoch 171: Train Loss = 2.8111, Val Loss = 2.8206\n",
            "Epoch 172: Train Loss = 2.8081, Val Loss = 2.8176\n",
            "Epoch 173: Train Loss = 2.8051, Val Loss = 2.8146\n",
            "Epoch 174: Train Loss = 2.8021, Val Loss = 2.8116\n",
            "Epoch 175: Train Loss = 2.7992, Val Loss = 2.8086\n",
            "Epoch 176: Train Loss = 2.7962, Val Loss = 2.8056\n",
            "Epoch 177: Train Loss = 2.7932, Val Loss = 2.8026\n",
            "Epoch 178: Train Loss = 2.7903, Val Loss = 2.7996\n",
            "Epoch 179: Train Loss = 2.7873, Val Loss = 2.7966\n",
            "Epoch 180: Train Loss = 2.7843, Val Loss = 2.7937\n",
            "Epoch 181: Train Loss = 2.7814, Val Loss = 2.7907\n",
            "Epoch 182: Train Loss = 2.7784, Val Loss = 2.7877\n",
            "Epoch 183: Train Loss = 2.7755, Val Loss = 2.7847\n",
            "Epoch 184: Train Loss = 2.7725, Val Loss = 2.7818\n",
            "Epoch 185: Train Loss = 2.7696, Val Loss = 2.7788\n",
            "Epoch 186: Train Loss = 2.7667, Val Loss = 2.7759\n",
            "Epoch 187: Train Loss = 2.7637, Val Loss = 2.7729\n",
            "Epoch 188: Train Loss = 2.7608, Val Loss = 2.7700\n",
            "Epoch 189: Train Loss = 2.7579, Val Loss = 2.7671\n",
            "Epoch 190: Train Loss = 2.7550, Val Loss = 2.7641\n",
            "Epoch 191: Train Loss = 2.7521, Val Loss = 2.7612\n",
            "Epoch 192: Train Loss = 2.7492, Val Loss = 2.7583\n",
            "Epoch 193: Train Loss = 2.7463, Val Loss = 2.7554\n",
            "Epoch 194: Train Loss = 2.7434, Val Loss = 2.7524\n",
            "Epoch 195: Train Loss = 2.7405, Val Loss = 2.7495\n",
            "Epoch 196: Train Loss = 2.7376, Val Loss = 2.7466\n",
            "Epoch 197: Train Loss = 2.7347, Val Loss = 2.7437\n",
            "Epoch 198: Train Loss = 2.7318, Val Loss = 2.7408\n",
            "Epoch 199: Train Loss = 2.7290, Val Loss = 2.7379\n",
            "Epoch 200: Train Loss = 2.7260, Val Loss = 2.7350\n",
            "Epoch 201: Train Loss = 2.7231, Val Loss = 2.7321\n",
            "Epoch 202: Train Loss = 2.7202, Val Loss = 2.7292\n",
            "Epoch 203: Train Loss = 2.7173, Val Loss = 2.7263\n",
            "Epoch 204: Train Loss = 2.7144, Val Loss = 2.7234\n",
            "Epoch 205: Train Loss = 2.7115, Val Loss = 2.7205\n",
            "Epoch 206: Train Loss = 2.7086, Val Loss = 2.7176\n",
            "Epoch 207: Train Loss = 2.7057, Val Loss = 2.7148\n",
            "Epoch 208: Train Loss = 2.7028, Val Loss = 2.7119\n",
            "Epoch 209: Train Loss = 2.6999, Val Loss = 2.7090\n",
            "Epoch 210: Train Loss = 2.6971, Val Loss = 2.7061\n",
            "Epoch 211: Train Loss = 2.6942, Val Loss = 2.7033\n",
            "Epoch 212: Train Loss = 2.6913, Val Loss = 2.7004\n",
            "Epoch 213: Train Loss = 2.6885, Val Loss = 2.6975\n",
            "Epoch 214: Train Loss = 2.6856, Val Loss = 2.6947\n",
            "Epoch 215: Train Loss = 2.6827, Val Loss = 2.6918\n",
            "Epoch 216: Train Loss = 2.6799, Val Loss = 2.6890\n",
            "Epoch 217: Train Loss = 2.6770, Val Loss = 2.6861\n",
            "Epoch 218: Train Loss = 2.6742, Val Loss = 2.6833\n",
            "Epoch 219: Train Loss = 2.6713, Val Loss = 2.6805\n",
            "Epoch 220: Train Loss = 2.6685, Val Loss = 2.6776\n",
            "Epoch 221: Train Loss = 2.6657, Val Loss = 2.6748\n",
            "Epoch 222: Train Loss = 2.6628, Val Loss = 2.6720\n",
            "Epoch 223: Train Loss = 2.6600, Val Loss = 2.6691\n",
            "Epoch 224: Train Loss = 2.6572, Val Loss = 2.6663\n",
            "Epoch 225: Train Loss = 2.6543, Val Loss = 2.6635\n",
            "Epoch 226: Train Loss = 2.6515, Val Loss = 2.6607\n",
            "Epoch 227: Train Loss = 2.6487, Val Loss = 2.6579\n",
            "Epoch 228: Train Loss = 2.6459, Val Loss = 2.6551\n",
            "Epoch 229: Train Loss = 2.6431, Val Loss = 2.6523\n",
            "Epoch 230: Train Loss = 2.6403, Val Loss = 2.6495\n",
            "Epoch 231: Train Loss = 2.6375, Val Loss = 2.6467\n",
            "Epoch 232: Train Loss = 2.6347, Val Loss = 2.6439\n",
            "Epoch 233: Train Loss = 2.6319, Val Loss = 2.6411\n",
            "Epoch 234: Train Loss = 2.6291, Val Loss = 2.6384\n",
            "Epoch 235: Train Loss = 2.6263, Val Loss = 2.6356\n",
            "Epoch 236: Train Loss = 2.6235, Val Loss = 2.6328\n",
            "Epoch 237: Train Loss = 2.6207, Val Loss = 2.6301\n",
            "Epoch 238: Train Loss = 2.6180, Val Loss = 2.6273\n",
            "Epoch 239: Train Loss = 2.6152, Val Loss = 2.6245\n",
            "Epoch 240: Train Loss = 2.6124, Val Loss = 2.6218\n",
            "Epoch 241: Train Loss = 2.6097, Val Loss = 2.6190\n",
            "Epoch 242: Train Loss = 2.6069, Val Loss = 2.6163\n",
            "Epoch 243: Train Loss = 2.6042, Val Loss = 2.6135\n",
            "Epoch 244: Train Loss = 2.6014, Val Loss = 2.6108\n",
            "Epoch 245: Train Loss = 2.5987, Val Loss = 2.6081\n",
            "Epoch 246: Train Loss = 2.5959, Val Loss = 2.6053\n",
            "Epoch 247: Train Loss = 2.5932, Val Loss = 2.6026\n",
            "Epoch 248: Train Loss = 2.5905, Val Loss = 2.5999\n",
            "Epoch 249: Train Loss = 2.5877, Val Loss = 2.5972\n",
            "Epoch 250: Train Loss = 2.5850, Val Loss = 2.5945\n",
            "Epoch 251: Train Loss = 2.5823, Val Loss = 2.5918\n",
            "Epoch 252: Train Loss = 2.5796, Val Loss = 2.5891\n",
            "Epoch 253: Train Loss = 2.5769, Val Loss = 2.5864\n",
            "Epoch 254: Train Loss = 2.5741, Val Loss = 2.5837\n",
            "Epoch 255: Train Loss = 2.5714, Val Loss = 2.5810\n",
            "Epoch 256: Train Loss = 2.5687, Val Loss = 2.5783\n",
            "Epoch 257: Train Loss = 2.5660, Val Loss = 2.5756\n",
            "Epoch 258: Train Loss = 2.5634, Val Loss = 2.5729\n",
            "Epoch 259: Train Loss = 2.5607, Val Loss = 2.5703\n",
            "Epoch 260: Train Loss = 2.5580, Val Loss = 2.5676\n",
            "Epoch 261: Train Loss = 2.5553, Val Loss = 2.5649\n",
            "Epoch 262: Train Loss = 2.5526, Val Loss = 2.5623\n",
            "Epoch 263: Train Loss = 2.5500, Val Loss = 2.5596\n",
            "Epoch 264: Train Loss = 2.5473, Val Loss = 2.5569\n",
            "Epoch 265: Train Loss = 2.5446, Val Loss = 2.5543\n",
            "Epoch 266: Train Loss = 2.5420, Val Loss = 2.5517\n",
            "Epoch 267: Train Loss = 2.5393, Val Loss = 2.5490\n",
            "Epoch 268: Train Loss = 2.5366, Val Loss = 2.5464\n",
            "Epoch 269: Train Loss = 2.5340, Val Loss = 2.5437\n",
            "Epoch 270: Train Loss = 2.5314, Val Loss = 2.5411\n",
            "Epoch 271: Train Loss = 2.5287, Val Loss = 2.5385\n",
            "Epoch 272: Train Loss = 2.5261, Val Loss = 2.5359\n",
            "Epoch 273: Train Loss = 2.5234, Val Loss = 2.5332\n",
            "Epoch 274: Train Loss = 2.5208, Val Loss = 2.5306\n",
            "Epoch 275: Train Loss = 2.5182, Val Loss = 2.5280\n",
            "Epoch 276: Train Loss = 2.5156, Val Loss = 2.5254\n",
            "Epoch 277: Train Loss = 2.5130, Val Loss = 2.5228\n",
            "Epoch 278: Train Loss = 2.5103, Val Loss = 2.5202\n",
            "Epoch 279: Train Loss = 2.5077, Val Loss = 2.5176\n",
            "Epoch 280: Train Loss = 2.5051, Val Loss = 2.5150\n",
            "Epoch 281: Train Loss = 2.5025, Val Loss = 2.5125\n",
            "Epoch 282: Train Loss = 2.4999, Val Loss = 2.5099\n",
            "Epoch 283: Train Loss = 2.4974, Val Loss = 2.5073\n",
            "Epoch 284: Train Loss = 2.4948, Val Loss = 2.5047\n",
            "Epoch 285: Train Loss = 2.4922, Val Loss = 2.5022\n",
            "Epoch 286: Train Loss = 2.4896, Val Loss = 2.4996\n",
            "Epoch 287: Train Loss = 2.4870, Val Loss = 2.4970\n",
            "Epoch 288: Train Loss = 2.4845, Val Loss = 2.4945\n",
            "Epoch 289: Train Loss = 2.4819, Val Loss = 2.4919\n",
            "Epoch 290: Train Loss = 2.4793, Val Loss = 2.4894\n",
            "Epoch 291: Train Loss = 2.4768, Val Loss = 2.4868\n",
            "Epoch 292: Train Loss = 2.4742, Val Loss = 2.4843\n",
            "Epoch 293: Train Loss = 2.4717, Val Loss = 2.4818\n",
            "Epoch 294: Train Loss = 2.4691, Val Loss = 2.4792\n",
            "Epoch 295: Train Loss = 2.4666, Val Loss = 2.4767\n",
            "Epoch 296: Train Loss = 2.4640, Val Loss = 2.4742\n",
            "Epoch 297: Train Loss = 2.4615, Val Loss = 2.4717\n",
            "Epoch 298: Train Loss = 2.4590, Val Loss = 2.4691\n",
            "Epoch 299: Train Loss = 2.4564, Val Loss = 2.4666\n",
            "Epoch 300: Train Loss = 2.4539, Val Loss = 2.4641\n",
            "Epoch 301: Train Loss = 2.4514, Val Loss = 2.4616\n",
            "Epoch 302: Train Loss = 2.4489, Val Loss = 2.4591\n",
            "Epoch 303: Train Loss = 2.4463, Val Loss = 2.4566\n",
            "Epoch 304: Train Loss = 2.4438, Val Loss = 2.4541\n",
            "Epoch 305: Train Loss = 2.4413, Val Loss = 2.4516\n",
            "Epoch 306: Train Loss = 2.4388, Val Loss = 2.4492\n",
            "Epoch 307: Train Loss = 2.4363, Val Loss = 2.4467\n",
            "Epoch 308: Train Loss = 2.4338, Val Loss = 2.4442\n",
            "Epoch 309: Train Loss = 2.4313, Val Loss = 2.4417\n",
            "Epoch 310: Train Loss = 2.4289, Val Loss = 2.4393\n",
            "Epoch 311: Train Loss = 2.4264, Val Loss = 2.4368\n",
            "Epoch 312: Train Loss = 2.4239, Val Loss = 2.4343\n",
            "Epoch 313: Train Loss = 2.4214, Val Loss = 2.4319\n",
            "Epoch 314: Train Loss = 2.4189, Val Loss = 2.4294\n",
            "Epoch 315: Train Loss = 2.4165, Val Loss = 2.4270\n",
            "Epoch 316: Train Loss = 2.4140, Val Loss = 2.4245\n",
            "Epoch 317: Train Loss = 2.4115, Val Loss = 2.4221\n",
            "Epoch 318: Train Loss = 2.4091, Val Loss = 2.4196\n",
            "Epoch 319: Train Loss = 2.4066, Val Loss = 2.4172\n",
            "Epoch 320: Train Loss = 2.4042, Val Loss = 2.4148\n",
            "Epoch 321: Train Loss = 2.4017, Val Loss = 2.4123\n",
            "Epoch 322: Train Loss = 2.3993, Val Loss = 2.4099\n",
            "Epoch 323: Train Loss = 2.3969, Val Loss = 2.4075\n",
            "Epoch 324: Train Loss = 2.3944, Val Loss = 2.4051\n",
            "Epoch 325: Train Loss = 2.3920, Val Loss = 2.4027\n",
            "Epoch 326: Train Loss = 2.3896, Val Loss = 2.4003\n",
            "Epoch 327: Train Loss = 2.3871, Val Loss = 2.3979\n",
            "Epoch 328: Train Loss = 2.3847, Val Loss = 2.3955\n",
            "Epoch 329: Train Loss = 2.3823, Val Loss = 2.3931\n",
            "Epoch 330: Train Loss = 2.3799, Val Loss = 2.3907\n",
            "Epoch 331: Train Loss = 2.3775, Val Loss = 2.3883\n",
            "Epoch 332: Train Loss = 2.3751, Val Loss = 2.3859\n",
            "Epoch 333: Train Loss = 2.3727, Val Loss = 2.3835\n",
            "Epoch 334: Train Loss = 2.3703, Val Loss = 2.3811\n",
            "Epoch 335: Train Loss = 2.3679, Val Loss = 2.3788\n",
            "Epoch 336: Train Loss = 2.3655, Val Loss = 2.3764\n",
            "Epoch 337: Train Loss = 2.3631, Val Loss = 2.3740\n",
            "Epoch 338: Train Loss = 2.3607, Val Loss = 2.3716\n",
            "Epoch 339: Train Loss = 2.3583, Val Loss = 2.3693\n",
            "Epoch 340: Train Loss = 2.3560, Val Loss = 2.3669\n",
            "Epoch 341: Train Loss = 2.3536, Val Loss = 2.3646\n",
            "Epoch 342: Train Loss = 2.3512, Val Loss = 2.3622\n",
            "Epoch 343: Train Loss = 2.3488, Val Loss = 2.3599\n",
            "Epoch 344: Train Loss = 2.3465, Val Loss = 2.3575\n",
            "Epoch 345: Train Loss = 2.3441, Val Loss = 2.3552\n",
            "Epoch 346: Train Loss = 2.3418, Val Loss = 2.3529\n",
            "Epoch 347: Train Loss = 2.3394, Val Loss = 2.3505\n",
            "Epoch 348: Train Loss = 2.3371, Val Loss = 2.3482\n",
            "Epoch 349: Train Loss = 2.3347, Val Loss = 2.3459\n",
            "Epoch 350: Train Loss = 2.3324, Val Loss = 2.3436\n",
            "Epoch 351: Train Loss = 2.3301, Val Loss = 2.3413\n",
            "Epoch 352: Train Loss = 2.3277, Val Loss = 2.3389\n",
            "Epoch 353: Train Loss = 2.3254, Val Loss = 2.3366\n",
            "Epoch 354: Train Loss = 2.3231, Val Loss = 2.3343\n",
            "Epoch 355: Train Loss = 2.3207, Val Loss = 2.3320\n",
            "Epoch 356: Train Loss = 2.3184, Val Loss = 2.3297\n",
            "Epoch 357: Train Loss = 2.3161, Val Loss = 2.3274\n",
            "Epoch 358: Train Loss = 2.3138, Val Loss = 2.3251\n",
            "Epoch 359: Train Loss = 2.3115, Val Loss = 2.3229\n",
            "Epoch 360: Train Loss = 2.3092, Val Loss = 2.3206\n",
            "Epoch 361: Train Loss = 2.3069, Val Loss = 2.3183\n",
            "Epoch 362: Train Loss = 2.3046, Val Loss = 2.3160\n",
            "Epoch 363: Train Loss = 2.3023, Val Loss = 2.3137\n",
            "Epoch 364: Train Loss = 2.3000, Val Loss = 2.3115\n",
            "Epoch 365: Train Loss = 2.2977, Val Loss = 2.3092\n",
            "Epoch 366: Train Loss = 2.2954, Val Loss = 2.3069\n",
            "Epoch 367: Train Loss = 2.2931, Val Loss = 2.3047\n",
            "Epoch 368: Train Loss = 2.2909, Val Loss = 2.3024\n",
            "Epoch 369: Train Loss = 2.2886, Val Loss = 2.3002\n",
            "Epoch 370: Train Loss = 2.2863, Val Loss = 2.2979\n",
            "Epoch 371: Train Loss = 2.2841, Val Loss = 2.2957\n",
            "Epoch 372: Train Loss = 2.2818, Val Loss = 2.2935\n",
            "Epoch 373: Train Loss = 2.2795, Val Loss = 2.2912\n",
            "Epoch 374: Train Loss = 2.2773, Val Loss = 2.2890\n",
            "Epoch 375: Train Loss = 2.2750, Val Loss = 2.2868\n",
            "Epoch 376: Train Loss = 2.2728, Val Loss = 2.2845\n",
            "Epoch 377: Train Loss = 2.2705, Val Loss = 2.2823\n",
            "Epoch 378: Train Loss = 2.2683, Val Loss = 2.2801\n",
            "Epoch 379: Train Loss = 2.2661, Val Loss = 2.2779\n",
            "Epoch 380: Train Loss = 2.2638, Val Loss = 2.2757\n",
            "Epoch 381: Train Loss = 2.2616, Val Loss = 2.2735\n",
            "Epoch 382: Train Loss = 2.2594, Val Loss = 2.2713\n",
            "Epoch 383: Train Loss = 2.2572, Val Loss = 2.2691\n",
            "Epoch 384: Train Loss = 2.2549, Val Loss = 2.2669\n",
            "Epoch 385: Train Loss = 2.2527, Val Loss = 2.2647\n",
            "Epoch 386: Train Loss = 2.2505, Val Loss = 2.2625\n",
            "Epoch 387: Train Loss = 2.2483, Val Loss = 2.2603\n",
            "Epoch 388: Train Loss = 2.2461, Val Loss = 2.2581\n",
            "Epoch 389: Train Loss = 2.2439, Val Loss = 2.2559\n",
            "Epoch 390: Train Loss = 2.2417, Val Loss = 2.2537\n",
            "Epoch 391: Train Loss = 2.2395, Val Loss = 2.2516\n",
            "Epoch 392: Train Loss = 2.2373, Val Loss = 2.2494\n",
            "Epoch 393: Train Loss = 2.2351, Val Loss = 2.2472\n",
            "Epoch 394: Train Loss = 2.2329, Val Loss = 2.2451\n",
            "Epoch 395: Train Loss = 2.2308, Val Loss = 2.2429\n",
            "Epoch 396: Train Loss = 2.2286, Val Loss = 2.2408\n",
            "Epoch 397: Train Loss = 2.2264, Val Loss = 2.2386\n",
            "Epoch 398: Train Loss = 2.2242, Val Loss = 2.2365\n",
            "Epoch 399: Train Loss = 2.2221, Val Loss = 2.2343\n",
            "Epoch 400: Train Loss = 2.2199, Val Loss = 2.2322\n",
            "Epoch 401: Train Loss = 2.2177, Val Loss = 2.2300\n",
            "Epoch 402: Train Loss = 2.2156, Val Loss = 2.2279\n",
            "Epoch 403: Train Loss = 2.2134, Val Loss = 2.2258\n",
            "Epoch 404: Train Loss = 2.2113, Val Loss = 2.2237\n",
            "Epoch 405: Train Loss = 2.2091, Val Loss = 2.2215\n",
            "Epoch 406: Train Loss = 2.2070, Val Loss = 2.2194\n",
            "Epoch 407: Train Loss = 2.2049, Val Loss = 2.2173\n",
            "Epoch 408: Train Loss = 2.2027, Val Loss = 2.2152\n",
            "Epoch 409: Train Loss = 2.2006, Val Loss = 2.2131\n",
            "Epoch 410: Train Loss = 2.1985, Val Loss = 2.2110\n",
            "Epoch 411: Train Loss = 2.1963, Val Loss = 2.2088\n",
            "Epoch 412: Train Loss = 2.1942, Val Loss = 2.2067\n",
            "Epoch 413: Train Loss = 2.1921, Val Loss = 2.2046\n",
            "Epoch 414: Train Loss = 2.1900, Val Loss = 2.2026\n",
            "Epoch 415: Train Loss = 2.1879, Val Loss = 2.2005\n",
            "Epoch 416: Train Loss = 2.1857, Val Loss = 2.1984\n",
            "Epoch 417: Train Loss = 2.1836, Val Loss = 2.1963\n",
            "Epoch 418: Train Loss = 2.1815, Val Loss = 2.1942\n",
            "Epoch 419: Train Loss = 2.1794, Val Loss = 2.1921\n",
            "Epoch 420: Train Loss = 2.1773, Val Loss = 2.1900\n",
            "Epoch 421: Train Loss = 2.1753, Val Loss = 2.1880\n",
            "Epoch 422: Train Loss = 2.1732, Val Loss = 2.1859\n",
            "Epoch 423: Train Loss = 2.1711, Val Loss = 2.1838\n",
            "Epoch 424: Train Loss = 2.1690, Val Loss = 2.1818\n",
            "Epoch 425: Train Loss = 2.1669, Val Loss = 2.1797\n",
            "Epoch 426: Train Loss = 2.1648, Val Loss = 2.1777\n",
            "Epoch 427: Train Loss = 2.1628, Val Loss = 2.1756\n",
            "Epoch 428: Train Loss = 2.1607, Val Loss = 2.1736\n",
            "Epoch 429: Train Loss = 2.1586, Val Loss = 2.1715\n",
            "Epoch 430: Train Loss = 2.1566, Val Loss = 2.1695\n",
            "Epoch 431: Train Loss = 2.1545, Val Loss = 2.1674\n",
            "Epoch 432: Train Loss = 2.1525, Val Loss = 2.1654\n",
            "Epoch 433: Train Loss = 2.1504, Val Loss = 2.1634\n",
            "Epoch 434: Train Loss = 2.1484, Val Loss = 2.1614\n",
            "Epoch 435: Train Loss = 2.1463, Val Loss = 2.1593\n",
            "Epoch 436: Train Loss = 2.1443, Val Loss = 2.1573\n",
            "Epoch 437: Train Loss = 2.1422, Val Loss = 2.1553\n",
            "Epoch 438: Train Loss = 2.1402, Val Loss = 2.1533\n",
            "Epoch 439: Train Loss = 2.1382, Val Loss = 2.1513\n",
            "Epoch 440: Train Loss = 2.1361, Val Loss = 2.1493\n",
            "Epoch 441: Train Loss = 2.1341, Val Loss = 2.1472\n",
            "Epoch 442: Train Loss = 2.1321, Val Loss = 2.1452\n",
            "Epoch 443: Train Loss = 2.1301, Val Loss = 2.1432\n",
            "Epoch 444: Train Loss = 2.1281, Val Loss = 2.1413\n",
            "Epoch 445: Train Loss = 2.1260, Val Loss = 2.1393\n",
            "Epoch 446: Train Loss = 2.1240, Val Loss = 2.1373\n",
            "Epoch 447: Train Loss = 2.1220, Val Loss = 2.1353\n",
            "Epoch 448: Train Loss = 2.1200, Val Loss = 2.1333\n",
            "Epoch 449: Train Loss = 2.1180, Val Loss = 2.1313\n",
            "Epoch 450: Train Loss = 2.1160, Val Loss = 2.1294\n",
            "Epoch 451: Train Loss = 2.1140, Val Loss = 2.1274\n",
            "Epoch 452: Train Loss = 2.1121, Val Loss = 2.1254\n",
            "Epoch 453: Train Loss = 2.1101, Val Loss = 2.1235\n",
            "Epoch 454: Train Loss = 2.1081, Val Loss = 2.1215\n",
            "Epoch 455: Train Loss = 2.1061, Val Loss = 2.1195\n",
            "Epoch 456: Train Loss = 2.1041, Val Loss = 2.1176\n",
            "Epoch 457: Train Loss = 2.1022, Val Loss = 2.1156\n",
            "Epoch 458: Train Loss = 2.1002, Val Loss = 2.1137\n",
            "Epoch 459: Train Loss = 2.0982, Val Loss = 2.1117\n",
            "Epoch 460: Train Loss = 2.0963, Val Loss = 2.1098\n",
            "Epoch 461: Train Loss = 2.0943, Val Loss = 2.1078\n",
            "Epoch 462: Train Loss = 2.0923, Val Loss = 2.1059\n",
            "Epoch 463: Train Loss = 2.0904, Val Loss = 2.1040\n",
            "Epoch 464: Train Loss = 2.0884, Val Loss = 2.1020\n",
            "Epoch 465: Train Loss = 2.0865, Val Loss = 2.1001\n",
            "Epoch 466: Train Loss = 2.0845, Val Loss = 2.0982\n",
            "Epoch 467: Train Loss = 2.0826, Val Loss = 2.0963\n",
            "Epoch 468: Train Loss = 2.0807, Val Loss = 2.0943\n",
            "Epoch 469: Train Loss = 2.0787, Val Loss = 2.0924\n",
            "Epoch 470: Train Loss = 2.0768, Val Loss = 2.0905\n",
            "Epoch 471: Train Loss = 2.0749, Val Loss = 2.0886\n",
            "Epoch 472: Train Loss = 2.0729, Val Loss = 2.0867\n",
            "Epoch 473: Train Loss = 2.0710, Val Loss = 2.0848\n",
            "Epoch 474: Train Loss = 2.0691, Val Loss = 2.0829\n",
            "Epoch 475: Train Loss = 2.0672, Val Loss = 2.0810\n",
            "Epoch 476: Train Loss = 2.0653, Val Loss = 2.0791\n",
            "Epoch 477: Train Loss = 2.0634, Val Loss = 2.0772\n",
            "Epoch 478: Train Loss = 2.0615, Val Loss = 2.0753\n",
            "Epoch 479: Train Loss = 2.0596, Val Loss = 2.0734\n",
            "Epoch 480: Train Loss = 2.0577, Val Loss = 2.0715\n",
            "Epoch 481: Train Loss = 2.0558, Val Loss = 2.0697\n",
            "Epoch 482: Train Loss = 2.0539, Val Loss = 2.0678\n",
            "Epoch 483: Train Loss = 2.0520, Val Loss = 2.0659\n",
            "Epoch 484: Train Loss = 2.0501, Val Loss = 2.0641\n",
            "Epoch 485: Train Loss = 2.0482, Val Loss = 2.0622\n",
            "Epoch 486: Train Loss = 2.0463, Val Loss = 2.0603\n",
            "Epoch 487: Train Loss = 2.0444, Val Loss = 2.0585\n",
            "Epoch 488: Train Loss = 2.0426, Val Loss = 2.0566\n",
            "Epoch 489: Train Loss = 2.0407, Val Loss = 2.0548\n",
            "Epoch 490: Train Loss = 2.0388, Val Loss = 2.0529\n",
            "Epoch 491: Train Loss = 2.0370, Val Loss = 2.0511\n",
            "Epoch 492: Train Loss = 2.0351, Val Loss = 2.0492\n",
            "Epoch 493: Train Loss = 2.0332, Val Loss = 2.0474\n",
            "Epoch 494: Train Loss = 2.0314, Val Loss = 2.0455\n",
            "Epoch 495: Train Loss = 2.0295, Val Loss = 2.0437\n",
            "Epoch 496: Train Loss = 2.0277, Val Loss = 2.0419\n",
            "Epoch 497: Train Loss = 2.0258, Val Loss = 2.0400\n",
            "Epoch 498: Train Loss = 2.0240, Val Loss = 2.0382\n",
            "Epoch 499: Train Loss = 2.0222, Val Loss = 2.0364\n",
            "Epoch 500: Train Loss = 2.0203, Val Loss = 2.0346\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "model, train_loss_per_epoch, val_loss_per_epoch = fit(model, train_ds, val_ds, batch_size, alpha, epochs)\n",
        "# model, train_loss, val_loss = fit(model, train_ds, train_ds, batch_size, alpha, epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c41885",
      "metadata": {
        "id": "45c41885"
      },
      "source": [
        "#### Display the training loss and validation loss against epoch graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert TensorFlow tensors to plain Python lists\n",
        "train_loss_list = [float(x) for x in train_loss_per_epoch]\n",
        "val_loss_list = [float(x) for x in val_loss_per_epoch]\n",
        "\n",
        "# Sample x values (epochs)\n",
        "epochs = list(range(1, len(train_loss_list) + 1))\n",
        "\n",
        "# Create data sources for hover\n",
        "source = ColumnDataSource(data={'epoch': epochs, 'train_loss': train_loss_list, 'val_loss': val_loss_list})\n",
        "\n",
        "# Create figure\n",
        "p = figure(title=\"Training and Validation Loss\", x_axis_label='Epoch', y_axis_label='Loss', width=900, height=600)\n",
        "p.line('epoch', 'train_loss', source=source, legend_label=\"Training Loss\", line_width=2, color=\"blue\", muted_alpha=0.1)\n",
        "p.line('epoch', 'val_loss', source=source, legend_label=\"Validation Loss\", line_width=2, color=\"red\", muted_alpha=0.1)\n",
        "\n",
        "# Add hover tool\n",
        "hover = HoverTool(tooltips=[(\"Epoch\", \"@epoch\"), (\"Training Loss\", \"@train_loss\"), (\"Validation Loss\", \"@val_loss\")])\n",
        "p.add_tools(hover)\n",
        "p.legend.location = \"top_right\"\n",
        "p.legend.click_policy = \"mute\"  # Enables toggling visibility on click\n",
        "\n",
        "# Show plot\n",
        "show(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Lu9dCKjtUM6m",
        "outputId": "8d3951fc-6ad2-4d88-da4b-baa26774d4a8"
      },
      "id": "Lu9dCKjtUM6m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "'use strict';\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  const force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "const JS_MIME_TYPE = 'application/javascript';\n",
              "  const HTML_MIME_TYPE = 'text/html';\n",
              "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    const script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    function drop(id) {\n",
              "      const view = Bokeh.index.get_by_id(id)\n",
              "      if (view != null) {\n",
              "        view.model.document.clear()\n",
              "        Bokeh.index.delete(view)\n",
              "      }\n",
              "    }\n",
              "\n",
              "    const cell = handle.cell;\n",
              "\n",
              "    const id = cell.output_area._bokeh_element_id;\n",
              "    const server_id = cell.output_area._bokeh_server_id;\n",
              "\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null) {\n",
              "      drop(id)\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd_clean, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            const id = msg.content.text.trim()\n",
              "            drop(id)\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd_destroy);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    const output_area = handle.output_area;\n",
              "    const output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      const bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      const script_attrs = bk_div.children[0].attributes;\n",
              "      for (let i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      const toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    const events = require('base/js/events');\n",
              "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded(error = null) {\n",
              "    const el = document.getElementById(null);\n",
              "    if (el != null) {\n",
              "      const html = (() => {\n",
              "        if (typeof root.Bokeh === \"undefined\") {\n",
              "          if (error == null) {\n",
              "            return \"BokehJS is loading ...\";\n",
              "          } else {\n",
              "            return \"BokehJS failed to load.\";\n",
              "          }\n",
              "        } else {\n",
              "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
              "          if (error == null) {\n",
              "            return `${prefix} successfully loaded.`;\n",
              "          } else {\n",
              "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
              "          }\n",
              "        }\n",
              "      })();\n",
              "      el.innerHTML = html;\n",
              "\n",
              "      if (error != null) {\n",
              "        const wrapper = document.createElement(\"div\");\n",
              "        wrapper.style.overflow = \"auto\";\n",
              "        wrapper.style.height = \"5em\";\n",
              "        wrapper.style.resize = \"vertical\";\n",
              "        const content = document.createElement(\"div\");\n",
              "        content.style.fontFamily = \"monospace\";\n",
              "        content.style.whiteSpace = \"pre-wrap\";\n",
              "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
              "        content.textContent = error.stack ?? error.toString();\n",
              "        wrapper.append(content);\n",
              "        el.append(wrapper);\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(() => display_loaded(error), 100);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error(url) {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n",
              "  const css_urls = [];\n",
              "\n",
              "  const inline_js = [    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "      try {\n",
              "            for (let i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "\n",
              "      } catch (error) {throw error;\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(null);\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {throw error;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"a58648b5-0b9a-41ff-bb71-30bf55d1d4d4\" data-root-id=\"p1006\" style=\"display: contents;\"></div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "  const docs_json = {\"93269695-2e02-4541-86c7-e870b6125ac6\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1006\",\"attributes\":{\"width\":900,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1007\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1008\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1016\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1017\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1009\",\"attributes\":{\"text\":\"Training and Validation Loss\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1047\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1003\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1004\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1005\"},\"data\":{\"type\":\"map\",\"entries\":[[\"epoch\",[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500]],[\"train_loss\",[3.378028154373169,3.3744256496429443,3.370828866958618,3.3672378063201904,3.3636538982391357,3.360048770904541,3.356443405151367,3.3528428077697754,3.349247455596924,3.3456573486328125,3.3420727252960205,3.338491678237915,3.3349175453186035,3.3313467502593994,3.327781915664673,3.3242223262786865,3.320626974105835,3.317033529281616,3.313445568084717,3.3098621368408203,3.306283473968506,3.3027102947235107,3.299126148223877,3.295518159866333,3.2919156551361084,3.288318395614624,3.284724473953247,3.2811338901519775,3.2775495052337646,3.273969888687134,3.2703959941864014,3.266826629638672,3.26326322555542,3.259704113006592,3.2561492919921875,3.252601146697998,3.249056339263916,3.2455174922943115,3.24198317527771,3.238454580307007,3.2349307537078857,3.2314095497131348,3.227891683578491,3.224378824234009,3.2208712100982666,3.2173678874969482,3.2138705253601074,3.2103772163391113,3.2068898677825928,3.2034060955047607,3.199928045272827,3.1964550018310547,3.192986249923706,3.1895241737365723,3.186065196990967,3.182612180709839,3.179163694381714,3.175719976425171,3.1722824573516846,3.168848752975464,3.1654205322265625,3.161985397338867,3.1585209369659424,3.155061721801758,3.1516077518463135,3.148160219192505,3.144716262817383,3.1412761211395264,3.1378402709960938,3.1344096660614014,3.13098406791687,3.1275622844696045,3.1241469383239746,3.1207375526428223,3.117333173751831,3.113933563232422,3.1105399131774902,3.107149839401245,3.1037652492523193,3.1003851890563965,3.097010374069214,3.093640089035034,3.090276002883911,3.0869157314300537,3.0835607051849365,3.080209970474243,3.076864242553711,3.0735232830047607,3.0701870918273926,3.0668561458587646,3.0635299682617188,3.0602080821990967,3.0568912029266357,3.053579330444336,3.0502724647521973,3.0469698905944824,3.043672561645508,3.040379285812378,3.037091016769409,3.0338079929351807,3.030529499053955,3.0272555351257324,3.023986339569092,3.020721912384033,3.0174620151519775,3.014206647872925,3.010955810546875,3.0077106952667236,3.004470109939575,3.0012338161468506,2.99800181388855,2.994774580001831,2.9915518760681152,2.9883344173431396,2.985121965408325,2.9819133281707764,2.9787096977233887,2.975510358810425,2.972315549850464,2.9691245555877686,2.965937376022339,2.962756633758545,2.9595797061920166,2.956406831741333,2.953238010406494,2.950071096420288,2.946908712387085,2.9437506198883057,2.940596342086792,2.9374477863311768,2.934303045272827,2.9311623573303223,2.9280266761779785,2.924896001815796,2.9217689037323,2.918647289276123,2.91552996635437,2.912386178970337,2.9092462062835693,2.906111001968384,2.902979612350464,2.899853229522705,2.8967323303222656,2.8936150074005127,2.8905012607574463,2.88739275932312,2.8842899799346924,2.881190061569214,2.8780956268310547,2.8750061988830566,2.871919870376587,2.8688385486602783,2.865762710571289,2.862689971923828,2.85962176322937,2.8565592765808105,2.8535001277923584,2.850444793701172,2.8473942279815674,2.844348430633545,2.841306209564209,2.838266611099243,2.8352322578430176,2.8322012424468994,2.8291752338409424,2.82615327835083,2.823137044906616,2.8201241493225098,2.8171160221099854,2.8141121864318848,2.8111138343811035,2.8081202507019043,2.805131196975708,2.8021464347839355,2.799165964126587,2.7961907386779785,2.7932188510894775,2.7902519702911377,2.7872893810272217,2.7843308448791504,2.781376600265503,2.7784268856048584,2.7754814624786377,2.772540330886841,2.769604206085205,2.766672372817993,2.763745069503784,2.7608225345611572,2.757904291152954,2.754990339279175,2.7520804405212402,2.7491753101348877,2.7462737560272217,2.743377447128296,2.7404847145080566,2.737596273422241,2.7347118854522705,2.7318308353424072,2.7289555072784424,2.726041078567505,2.7231247425079346,2.7202117443084717,2.717303991317749,2.714400053024292,2.711500883102417,2.7086055278778076,2.7057154178619385,2.702829360961914,2.699946880340576,2.6970677375793457,2.6941940784454346,2.691323757171631,2.68845534324646,2.6855907440185547,2.6827304363250732,2.679875135421753,2.677023410797119,2.6741766929626465,2.6713337898254395,2.668494462966919,2.6656534671783447,2.6628167629241943,2.659984827041626,2.657156467437744,2.654332399368286,2.651512861251831,2.648698329925537,2.6458871364593506,2.6430795192718506,2.6402766704559326,2.637479066848755,2.634683609008789,2.6318893432617188,2.629098653793335,2.626311779022217,2.623528242111206,2.620748996734619,2.6179747581481934,2.615204095840454,2.6124379634857178,2.6096761226654053,2.6069178581237793,2.6041646003723145,2.601414442062378,2.5986688137054443,2.5959277153015137,2.593190908432007,2.5904579162597656,2.5877292156219482,2.5850045680999756,2.5822832584381104,2.579566478729248,2.5768539905548096,2.5741450786590576,2.5714404582977295,2.5687406063079834,2.5660438537597656,2.563352108001709,2.560663938522339,2.5579795837402344,2.555299997329712,2.552624225616455,2.5499517917633057,2.5472829341888428,2.544617176055908,2.541956663131714,2.539299249649048,2.5366458892822266,2.533996820449829,2.5313515663146973,2.5287115573883057,2.526076078414917,2.5234436988830566,2.5208163261413574,2.518192768096924,2.515573740005493,2.512958526611328,2.510347843170166,2.5077404975891113,2.5051376819610596,2.5025386810302734,2.499943971633911,2.4973526000976562,2.4947659969329834,2.492182970046997,2.489603281021118,2.487027883529663,2.484457015991211,2.4818897247314453,2.4793262481689453,2.47676682472229,2.4742114543914795,2.4716591835021973,2.4691102504730225,2.4665653705596924,2.4640235900878906,2.461486339569092,2.4589531421661377,2.45642352104187,2.453897714614868,2.45137619972229,2.4488580226898193,2.4463443756103516,2.4438343048095703,2.4413278102874756,2.438825845718384,2.4363274574279785,2.433833360671997,2.431342601776123,2.4288554191589355,2.426372766494751,2.423893690109253,2.421417713165283,2.4189441204071045,2.4164745807647705,2.414008617401123,2.411545991897583,2.409088134765625,2.4066333770751953,2.4041826725006104,2.401735782623291,2.3992927074432373,2.3968536853790283,2.394418478012085,2.39198637008667,2.3895585536956787,2.3871350288391113,2.3847146034240723,2.382298231124878,2.37988543510437,2.3774771690368652,2.3750720024108887,2.3726704120635986,2.3702709674835205,2.367875337600708,2.365483522415161,2.363095998764038,2.3607115745544434,2.3583309650421143,2.3559539318084717,2.353581190109253,2.3512120246887207,2.348846197128296,2.346484661102295,2.3441262245178223,2.3417720794677734,2.339421033859253,2.337074041366577,2.334730863571167,2.3323912620544434,2.3300528526306152,2.327718734741211,2.325387954711914,2.3230607509613037,2.3207366466522217,2.3184168338775635,2.316100597381592,2.3137879371643066,2.311478853225708,2.309173583984375,2.3068721294403076,2.3045742511749268,2.3022801876068115,2.2999894618988037,2.2977025508880615,2.295419454574585,2.293139696121216,2.290863513946533,2.288591146469116,2.286322832107544,2.284057378768921,2.2817959785461426,2.27953839302063,2.2772841453552246,2.2750332355499268,2.2727861404418945,2.270542860031128,2.268303394317627,2.2660672664642334,2.2638347148895264,2.261605978012085,2.259380340576172,2.2571585178375244,2.254939079284668,2.2527225017547607,2.250509262084961,2.248300313949585,2.2460944652557373,2.243892192840576,2.2416930198669434,2.2394983768463135,2.237306594848633,2.2351183891296387,2.232933521270752,2.2307519912719727,2.228574752807617,2.226400136947632,2.2242300510406494,2.2220633029937744,2.2198996543884277,2.2177393436431885,2.215583086013794,2.213430166244507,2.211280345916748,2.209134340286255,2.2069919109344482,2.2048518657684326,2.2027151584625244,2.2005820274353027,2.1984522342681885,2.1963260173797607,2.1942026615142822,2.1920835971832275,2.1899678707122803,2.1878550052642822,2.18574595451355,2.183640241622925,2.181537628173828,2.1794381141662598,2.1773428916931152,2.17525053024292,2.173161745071411,2.1710762977600098,2.168994188308716,2.1669158935546875,2.1648406982421875,2.162768840789795,2.160700559616089,2.1586356163024902,2.1565744876861572,2.1545159816741943,2.152461528778076,2.1504099369049072,2.148362636566162,2.146317720413208,2.1442763805389404,2.1422383785247803,2.1402041912078857,2.138171672821045,2.1361422538757324,2.1341166496276855,2.132094144821167,2.130075216293335,2.1280596256256104,2.126047372817993,2.1240386962890625,2.122032880783081,2.1200320720672607,2.118034601211548,2.116039991378784,2.1140480041503906,2.112058639526367,2.110072135925293,2.108088970184326,2.106109380722046,2.104132890701294,2.1021595001220703,2.100188970565796,2.098221778869629,2.096257448196411,2.09429669380188,2.092339038848877,2.0903849601745605,2.0884339809417725,2.086486339569092,2.0845420360565186,2.0826010704040527,2.080662965774536,2.078728437423706,2.0767972469329834,2.074869155883789,2.072944402694702,2.0710225105285645,2.069103717803955,2.067188262939453,2.0652763843536377,2.0633671283721924,2.061461925506592,2.0595593452453613,2.05765962600708,2.055762529373169,2.0538692474365234,2.051978349685669,2.050091028213501,2.0482068061828613,2.046325445175171,2.044447660446167,2.0425732135772705,2.0407016277313232,2.0388333797454834,2.036968469619751,2.0351064205169678,2.033247709274292,2.0313923358917236,2.0295395851135254,2.0276906490325928,2.0258448123931885,2.0240018367767334,2.0221621990203857,2.0203254222869873]],[\"val_loss\",[3.3873507976531982,3.3837037086486816,3.380061626434326,3.376429319381714,3.3728065490722656,3.3691749572753906,3.3655476570129395,3.361924409866333,3.358307361602783,3.3546955585479736,3.3510890007019043,3.347487688064575,3.3438916206359863,3.3403007984161377,3.3367156982421875,3.3331356048583984,3.329537868499756,3.325946092605591,3.322359561920166,3.3187782764434814,3.315201997756958,3.311630964279175,3.3080642223358154,3.3044822216033936,3.300903797149658,3.2973320484161377,3.29376220703125,3.2901968955993652,3.2866382598876953,3.2830841541290283,3.279536485671997,3.2759933471679688,3.2724545001983643,3.2689223289489746,3.265393018722534,3.261870861053467,3.2583529949188232,3.25484037399292,3.251333236694336,3.247830867767334,3.2443344593048096,3.240837812423706,3.2373433113098145,3.233853816986084,3.2303688526153564,3.226889133453369,3.223414897918701,3.2199463844299316,3.2164814472198486,3.2130229473114014,3.209568738937378,3.206120014190674,3.202676296234131,3.1992383003234863,3.1958045959472656,3.192375898361206,3.188952922821045,3.185534715652466,3.182121992111206,3.1787140369415283,3.175311326980591,3.171884775161743,3.1684136390686035,3.164950132369995,3.161491870880127,3.1580398082733154,3.1545917987823486,3.1511478424072266,3.1477088928222656,3.1442744731903076,3.1408464908599854,3.1374213695526123,3.1340038776397705,3.130596399307251,3.1271939277648926,3.123795986175537,3.1204030513763428,3.1170153617858887,3.113633394241333,3.110255718231201,3.1068830490112305,3.103515148162842,3.1001534461975098,3.0967965126037598,3.0934441089630127,3.0900959968566895,3.086754322052002,3.0834169387817383,3.0800838470458984,3.0767571926116943,3.0734341144561768,3.0701167583465576,3.0668041706085205,3.0634963512420654,3.0601940155029297,3.056896686553955,3.053602695465088,3.0503153800964355,3.0470314025878906,3.0437541007995605,3.040480852127075,3.0372121334075928,3.0339484214782715,3.0306899547576904,3.0274362564086914,3.024186611175537,3.020941734313965,3.01770281791687,3.0144686698913574,3.0112383365631104,3.00801420211792,3.004793405532837,3.001577854156494,2.9983668327331543,2.995162010192871,2.9919610023498535,2.988764524459839,2.985572576522827,2.9823861122131348,2.9792022705078125,2.9760243892669678,2.9728519916534424,2.96968412399292,2.966520071029663,2.9633610248565674,2.9602041244506836,2.957052230834961,2.9539055824279785,2.950762987136841,2.947625160217285,2.9444921016693115,2.9413628578186035,2.938239574432373,2.9351205825805664,2.9320058822631836,2.928896427154541,2.925791025161743,2.9226248264312744,2.919459581375122,2.9162986278533936,2.913142681121826,2.90999174118042,2.906845808029175,2.9037041664123535,2.900566816329956,2.8974342346191406,2.8943073749542236,2.891183614730835,2.888066053390503,2.8849525451660156,2.8818438053131104,2.87873911857605,2.875640869140625,2.8725452423095703,2.869455099105835,2.8663697242736816,2.863288640975952,2.8602113723754883,2.857140064239502,2.8540725708007812,2.8510091304779053,2.847949504852295,2.844893455505371,2.8418421745300293,2.8387954235076904,2.8357532024383545,2.832716464996338,2.829683303833008,2.826655149459839,2.823631763458252,2.8206136226654053,2.8175997734069824,2.8145906925201416,2.811586380004883,2.8085861206054688,2.805591106414795,2.802600383758545,2.7996137142181396,2.7966322898864746,2.793654680252075,2.790681838989258,2.7877135276794434,2.7847495079040527,2.781790256500244,2.7788357734680176,2.775885820388794,2.772940158843994,2.7699990272521973,2.7670629024505615,2.7641308307647705,2.761202812194824,2.75827956199646,2.7553610801696777,2.7524466514587402,2.7495369911193848,2.746630907058716,2.7437291145324707,2.7408318519592285,2.7379391193389893,2.7350258827209473,2.73211669921875,2.7292113304138184,2.726311206817627,2.7234151363372803,2.7205233573913574,2.7176363468170166,2.7147538661956787,2.711876153945923,2.7090015411376953,2.7061312198638916,2.70326566696167,2.700404644012451,2.6975417137145996,2.69468355178833,2.691829204559326,2.6889796257019043,2.6861343383789062,2.6832938194274902,2.680457592010498,2.6776247024536133,2.674792528152466,2.6719651222229004,2.669142246246338,2.666323184967041,2.663508176803589,2.660698652267456,2.657893180847168,2.655092239379883,2.652294635772705,2.6495018005371094,2.6467137336730957,2.6439285278320312,2.641145706176758,2.6383681297302246,2.6355929374694824,2.632821798324585,2.6300554275512695,2.627293109893799,2.624535083770752,2.621781587600708,2.6190319061279297,2.6162869930267334,2.613546371459961,2.610809564590454,2.608076810836792,2.605348825454712,2.6026253700256348,2.599905014038086,2.5971903800964355,2.5944788455963135,2.591770648956299,2.589066982269287,2.58636736869812,2.583672046661377,2.580981492996216,2.5782947540283203,2.5756115913391113,2.572932720184326,2.5702590942382812,2.5675885677337646,2.564922571182251,2.5622611045837402,2.559603214263916,2.55694842338562,2.554298162460327,2.551652193069458,2.5490102767944336,2.546372413635254,2.543738842010498,2.5411086082458496,2.5384840965270996,2.5358633995056152,2.5332465171813965,2.5306339263916016,2.5280256271362305,2.525421619415283,2.5228214263916016,2.520226001739502,2.5176336765289307,2.515045642852783,2.5124621391296387,2.509882926940918,2.5073068141937256,2.5047354698181152,2.5021681785583496,2.4996044635772705,2.497044324874878,2.4944894313812256,2.4919376373291016,2.4893898963928223,2.486846446990967,2.484306812286377,2.481771469116211,2.479238986968994,2.476710319519043,2.4741859436035156,2.471665382385254,2.469149112701416,2.4666364192962646,2.464127779006958,2.461622953414917,2.4591224193573,2.456625461578369,2.454132556915283,2.451643466949463,2.4491584300994873,2.4466776847839355,2.4442007541656494,2.44172739982605,2.439257860183716,2.4367926120758057,2.434330940246582,2.431872844696045,2.429417848587036,2.426966905593872,2.4245200157165527,2.422076463699341,2.4196369647979736,2.417201519012451,2.4147696495056152,2.412341594696045,2.4099175930023193,2.4074978828430176,2.405081272125244,2.4026684761047363,2.4002599716186523,2.397855281829834,2.3954544067382812,2.393057346343994,2.3906636238098145,2.3882741928100586,2.3858883380889893,2.3835065364837646,2.38112735748291,2.3787522315979004,2.376380443572998,2.3740131855010986,2.3716490268707275,2.369288921356201,2.3669321537017822,2.364579677581787,2.3622310161590576,2.3598856925964355,2.357544422149658,2.3552067279815674,2.352872610092163,2.3505425453186035,2.3482158184051514,2.345893383026123,2.3435745239257812,2.3412578105926514,2.3389453887939453,2.3366358280181885,2.3343300819396973,2.3320274353027344,2.329728841781616,2.3274338245391846,2.3251426219940186,2.3228554725646973,2.320571184158325,2.318291187286377,2.3160147666931152,2.313742160797119,2.3114731311798096,2.3092079162597656,2.3069465160369873,2.3046886920928955,2.302434206008911,2.3001832962036133,2.2979369163513184,2.2956933975219727,2.2934539318084717,2.291217803955078,2.288985252380371,2.2867562770843506,2.2845308780670166,2.2823095321655273,2.2800917625427246,2.2778773307800293,2.2756664752960205,2.2734594345092773,2.2712554931640625,2.2690556049346924,2.2668583393096924,2.2646641731262207,2.2624735832214355,2.2602875232696533,2.2581043243408203,2.255924701690674,2.2537484169006348,2.2515757083892822,2.2494068145751953,2.247241497039795,2.2450790405273438,2.242920398712158,2.240765333175659,2.2386136054992676,2.2364661693573,2.2343215942382812,2.2321808338165283,2.2300429344177246,2.2279093265533447,2.2257790565490723,2.22365140914917,2.2215278148651123,2.2194080352783203,2.2172889709472656,2.2151730060577393,2.2130606174468994,2.210952043533325,2.2088463306427,2.2067437171936035,2.2046456336975098,2.2025504112243652,2.2004587650299072,2.1983704566955566,2.1962850093841553,2.1942028999328613,2.192124128341675,2.190049409866333,2.1879773139953613,2.1859090328216553,2.1838436126708984,2.1817822456359863,2.1797242164611816,2.177669048309326,2.1756176948547363,2.173569440841675,2.1715247631073,2.1694836616516113,2.167445659637451,2.1654109954833984,2.1633801460266113,2.1613523960113525,2.159327745437622,2.157306432723999,2.1552886962890625,2.1532740592956543,2.1512625217437744,2.149254322052002,2.1472489833831787,2.145246982574463,2.1432483196258545,2.141253709793091,2.1392617225646973,2.1372733116149902,2.1352882385253906,2.1333072185516357,2.1313297748565674,2.1293551921844482,2.1273841857910156,2.125415325164795,2.1234500408172607,2.121487617492676,2.1195290088653564,2.1175732612609863,2.1156208515167236,2.113668918609619,2.1117208003997803,2.1097755432128906,2.1078336238861084,2.1058945655822754,2.103959560394287,2.10202693939209,2.100098133087158,2.098172664642334,2.096250057220459,2.0943310260772705,2.0924150943756104,2.0905022621154785,2.088593006134033,2.086686611175537,2.0847830772399902,2.08288311958313,2.0809860229492188,2.079092502593994,2.077202081680298,2.075314998626709,2.073430299758911,2.0715487003326416,2.069669723510742,2.06779408454895,2.0659215450286865,2.0640525817871094,2.0621862411499023,2.0603229999542236,2.0584628582000732,2.0566062927246094,2.0547525882720947,2.0529019832611084,2.0510547161102295,2.049210548400879,2.0473694801330566,2.045531749725342,2.043696641921997,2.0418648719787598,2.04003643989563,2.0382111072540283,2.036388635635376,2.034569263458252]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1048\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1049\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"train_loss\"},\"line_color\":\"blue\",\"line_width\":2}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1045\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"train_loss\"},\"line_color\":\"blue\",\"line_alpha\":0.1,\"line_width\":2}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1046\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"train_loss\"},\"line_color\":\"blue\",\"line_alpha\":0.1,\"line_width\":2}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1058\",\"attributes\":{\"data_source\":{\"id\":\"p1003\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1059\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1060\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1055\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"val_loss\"},\"line_color\":\"red\",\"line_width\":2}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1056\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"val_loss\"},\"line_color\":\"red\",\"line_alpha\":0.1,\"line_width\":2}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1057\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"epoch\"},\"y\":{\"type\":\"field\",\"field\":\"val_loss\"},\"line_color\":\"red\",\"line_alpha\":0.1,\"line_width\":2}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1015\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1028\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1029\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1030\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1031\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1037\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1036\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1038\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1039\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1040\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1062\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Epoch\",\"@epoch\"],[\"Training Loss\",\"@train_loss\"],[\"Validation Loss\",\"@val_loss\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1023\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1024\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1025\"},\"axis_label\":\"Loss\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1026\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1018\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1019\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1020\"},\"axis_label\":\"Epoch\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1021\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1022\",\"attributes\":{\"axis\":{\"id\":\"p1018\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1027\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1023\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1050\",\"attributes\":{\"click_policy\":\"mute\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1051\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Training Loss\"},\"renderers\":[{\"id\":\"p1047\"}]}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1061\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Validation Loss\"},\"renderers\":[{\"id\":\"p1058\"}]}}]}}]}}]}};\n",
              "  const render_items = [{\"docid\":\"93269695-2e02-4541-86c7-e870b6125ac6\",\"roots\":{\"p1006\":\"a58648b5-0b9a-41ff-bb71-30bf55d1d4d4\"},\"root_ids\":[\"p1006\"]}];\n",
              "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    let attempts = 0;\n",
              "    const timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else {\n",
              "        attempts++;\n",
              "        if (attempts > 100) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "p1006"
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0b4c38",
      "metadata": {
        "id": "0c0b4c38"
      },
      "source": [
        "#### Predict the test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict (model, test_ds):\n",
        "  \"\"\"\n",
        "  This function predicts the test set.\n",
        "  \"\"\"\n",
        "  report=[]\n",
        "  for x_batch_test, y_batch_test in test_ds.batch(20):\n",
        "    yhat = forward(model[0], model[1], x_batch_test)\n",
        "    J = loss_fn(y_batch_test, yhat)\n",
        "    yhat = tf.cast(yhat > 0.5, tf.int32)  # now we can hard cast yhat to return 0 or 1\n",
        "    for i in range(x_batch_test.shape[0]):\n",
        "      report.append([*x_batch_test.numpy()[i], y_batch_test.numpy()[i], yhat.numpy()[i], J.numpy()])\n",
        "  report = pd.DataFrame(report, columns=[*X.columns, \"Actual label\", \"Predicted label\", \"Loss\"])\n",
        "  return report"
      ],
      "metadata": {
        "id": "e3ft5DLTSpjA"
      },
      "id": "e3ft5DLTSpjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84f73b1",
      "metadata": {
        "id": "a84f73b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb340ba8-1cf7-4164-e71b-e9c606bd30d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           f1        f2        f3        f4        f5  Actual label  \\\n",
            "0   -1.138573 -2.516707  0.289952 -0.926953  1.180336             1   \n",
            "1    1.264300 -0.133768  1.005905 -0.614461 -1.363974             0   \n",
            "2    0.834665  0.762059  1.043679 -0.526401 -0.330217             0   \n",
            "3   -0.034206 -0.466700  0.110991 -0.212160 -1.429085             0   \n",
            "4   -0.612726 -0.745587  0.957615 -0.179273  0.828690             1   \n",
            "..        ...       ...       ...       ...       ...           ...   \n",
            "461  0.209144  0.428507  0.365459 -0.800480 -0.477457             0   \n",
            "462  0.590765  0.458360 -1.368133 -0.549997  0.305126             0   \n",
            "463 -0.187345 -0.724364 -0.150873  0.255468 -1.476060             1   \n",
            "464 -0.925007  1.144610 -0.424039 -0.785825  0.754091             0   \n",
            "465  0.050168 -0.192236 -0.092238  1.186582 -0.017933             1   \n",
            "\n",
            "     Predicted label      Loss  \n",
            "0                  1  1.968059  \n",
            "1                  1  1.968059  \n",
            "2                  1  1.968059  \n",
            "3                  0  1.968059  \n",
            "4                  1  1.968059  \n",
            "..               ...       ...  \n",
            "461                1  2.167862  \n",
            "462                0  2.167862  \n",
            "463                0  2.167862  \n",
            "464                0  2.167862  \n",
            "465                1  2.167862  \n",
            "\n",
            "[466 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "predictions = predict(model, test_ds)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d715ef",
      "metadata": {
        "id": "20d715ef"
      },
      "source": [
        "#### Display the confusion matrix and the classification report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs (training\n",
        "loss, validation loss,\n",
        "graph and model\n",
        "evaluation) are\n",
        "displayed."
      ],
      "metadata": {
        "id": "sDj5xY86k_YA"
      },
      "id": "sDj5xY86k_YA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35deeb3",
      "metadata": {
        "id": "c35deeb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ddac9f-530d-461a-e024-573f2928586a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.64      0.64       232\n",
            "           1       0.64      0.63      0.64       234\n",
            "\n",
            "    accuracy                           0.64       466\n",
            "   macro avg       0.64      0.64      0.64       466\n",
            "weighted avg       0.64      0.64      0.64       466\n",
            "\n",
            "Confusion matrix\n",
            "[[149  83]\n",
            " [ 86 148]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification report\")\n",
        "print(classification_report(predictions[\"Predicted label\"].tolist(), predictions[\"Actual label\"]))\n",
        "print(\"Confusion matrix\")\n",
        "print(confusion_matrix(predictions[\"Predicted label\"].tolist(), predictions[\"Actual label\"].tolist()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}